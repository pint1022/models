{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising a TensorFlow SavedModel for Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks shows how to optimise the TensorFlow exported SavedModel by **shrinking** its size (to have less memory and disk footprints), and **improving** prediction latency. This can be accopmlished by applying the following:\n",
    "* **Freezing**: That is, converting the variables stored in a checkpoint file of the SavedModel into constants stored directly in the model graph.\n",
    "* **Pruning**: That is, stripping unused nodes during the prediction path of the graph, merging duplicate nodes, as well as removing other node ops like summary, identity, etc.\n",
    "* **Quantisation**:  That is, converting any large float Const op into an eight-bit equivalent, followed by a float conversion op so that the result is usable by subsequent nodes.\n",
    "* **Other refinements**: That includes constant folding, batch_norm folding, fusing convolusion, etc.\n",
    "\n",
    "The optimisation operations we apply in this example are from the TensorFlow [Graph Conversion Tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#fold_constants), which is a c++ command-line tool. We use the Python APIs to call the c++ libraries. \n",
    "\n",
    "The Graph Transform Tool is designed to work on models that are saved as GraphDef files, usually in a binary protobuf format. However, the model exported after training and estimator is in SavedModel format (saved_model.pb file + variables folder with variables.data-* and variables.index files). \n",
    "\n",
    "We need to optimise the mode and keep it the SavedModel format. Thus, the optimisation steps will be:\n",
    "1. Freeze the SavedModel: SavedModel -> GraphDef\n",
    "2. Optimisae the freezed model: GraphDef -> GraphDef\n",
    "3. Convert the optimised freezed model to SavedModel: GraphDef -> SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow : 1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app as absl_app\n",
    "from absl import flags\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from official.mnist import dataset\n",
    "from official.utils.flags import core as flags_core\n",
    "from official.utils.logs import hooks_helper\n",
    "from official.utils.misc import distribution_utils\n",
    "from official.utils.misc import model_helpers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print (\"TensorFlow : {}\".format(tf.__version__))\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train and Export a TensorFlow DNNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0429 14:32:18.656301 30860 deprecation.py:323] From C:\\Users\\steve\\git\\models\\official\\mnist\\dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\n"
     ]
    }
   ],
   "source": [
    "MODELS_LOCATION = 'models\\mnist'\n",
    "MODEL_NAME = 'cnn_classifier'\n",
    "D_DIR= 'dataset'\n",
    "model_dir = os.path.join(MODELS_LOCATION, MODEL_NAME)\n",
    "datadir = os.path.join(MODELS_LOCATION, D_DIR)\n",
    "#train_data, train_labels = dataset.load_data(datadir)\n",
    "#(eval_data, eval_lebals, ts) = dataset.test(datadir)\n",
    "ds = dataset.train(datadir)\n",
    "es = dataset.test(datadir)\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = ds.batch(10)\n",
    "iter = images.make_one_shot_iterator()\n",
    "train_data, train_labels = iter.get_next()\n",
    "\n",
    "tests = es.batch(10)\n",
    "titer = tests.make_one_shot_iterator()\n",
    "eval_data, eval_labels = titer.get_next()\n",
    "\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    #sess.run(train_labels)\n",
    "    #sess.run(train_data)\n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        image = sess.run(train_data[i]).reshape(28,28)\n",
    "        label = sess.run(train_labels[i])\n",
    "        #print(\"image shape: {}, label {}\".format(image.shape, label))\n",
    "        plt.imshow(image, cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[label])\n",
    "    plt.show()\n",
    "\n",
    "#train_data = tf.cast(train_data, tf.float32)\n",
    "#eval_data = tf.cast(eval_data, tf.float32)\n",
    "#train_data = train_data.reshape(train_data.shape[0], train_data.shape[1]*train_data.shape[2])\n",
    "#eval_data = eval_data.reshape(eval_data.shape[0], eval_data.shape[1]*eval_data.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "def create_model(data_format):\n",
    "    \"\"\"Model to recognize digits in the MNIST dataset.\n",
    "    Network structure is equivalent to:\n",
    "    https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/examples/tutorials/mnist/mnist_deep.py\n",
    "    and\n",
    "    https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py\n",
    "    But uses the tf.keras API.\n",
    "    Args:\n",
    "      data_format: Either 'channels_first' or 'channels_last'. 'channels_first' is\n",
    "        typically faster on GPUs while 'channels_last' is typically faster on\n",
    "        CPUs. See\n",
    "        https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "    Returns:\n",
    "      A tf.keras.Model.\n",
    "    \"\"\"\n",
    "  \n",
    "    if data_format == 'channels_first':\n",
    "        input_shape = [1, 28, 28]\n",
    "    else:\n",
    "        assert data_format == 'channels_last'\n",
    "        input_shape = [28, 28, 1]\n",
    "\n",
    "    \n",
    "    l = tf.keras.layers\n",
    "    max_pool = l.MaxPooling2D(\n",
    "      (2, 2), (2, 2), padding='same', data_format=data_format)\n",
    "  # The model consists of a sequential chain of layers, so tf.keras.Sequential\n",
    "  # (a subclass of tf.keras.Model) makes for a compact description.\n",
    "  \n",
    "    return tf.keras.Sequential(\n",
    "      [\n",
    "          l.Reshape(\n",
    "              target_shape=input_shape,\n",
    "              input_shape=(28 * 28,)),\n",
    "          l.Conv2D(\n",
    "              32,\n",
    "              5,\n",
    "              padding='same',\n",
    "              data_format=data_format,\n",
    "              activation=tf.nn.relu),\n",
    "          max_pool,\n",
    "          l.Conv2D(\n",
    "              64,\n",
    "              5,\n",
    "              padding='same',\n",
    "              data_format=data_format,\n",
    "              activation=tf.nn.relu),\n",
    "          max_pool,\n",
    "          l.Flatten(),\n",
    "          l.Dense(512, activation=tf.nn.relu),\n",
    "          l.Dropout(0.4),\n",
    "          l.Dense(NUM_CLASSES)\n",
    "      ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"The model_fn argument for creating an Estimator.\"\"\"\n",
    "\n",
    "    print(\"params: {}\".format(params))\n",
    "    model = create_model(params['data_format'])\n",
    "    image = features\n",
    "    #pred_class = predictions['class_ids']\n",
    "    \n",
    "    if isinstance(image, dict):\n",
    "        image = features['image']\n",
    "\n",
    "    is_training = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "         \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        logits = model(image, training=False)\n",
    "        predicted_classes = tf.argmax(logits, 1)\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'classes': tf.argmax(logits, axis=1),\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "        }\n",
    "        #predictions=pred_class\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=tf.estimator.ModeKeys.PREDICT,\n",
    "            predictions=predictions,\n",
    "            export_outputs={\n",
    "                'predict': tf.estimator.export.PredictOutput(predictions)\n",
    "            })\n",
    "  \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        head = tf.contrib.estimator.multi_class_head(n_classes=NUM_CLASSES)   \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "\n",
    "        logits = model(image, training=True)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        accuracy = tf.metrics.accuracy(\n",
    "            labels=labels, predictions=tf.argmax(logits, axis=1))\n",
    "\n",
    "    # Name tensors to be logged with LoggingTensorHook.\n",
    "        tf.identity(LEARNING_RATE, 'learning_rate')\n",
    "        tf.identity(loss, 'cross_entropy')\n",
    "        tf.identity(accuracy[1], name='train_accuracy')\n",
    "\n",
    "    # Save accuracy scalar to Tensorboard output.\n",
    "        tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=tf.estimator.ModeKeys.TRAIN,\n",
    "            loss=loss,\n",
    "            train_op=optimizer.minimize(loss, tf.train.get_or_create_global_step()))\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        logits = model(image, training=False)\n",
    "        predicted_classes = tf.argmax(logits, 1)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=tf.estimator.ModeKeys.EVAL,\n",
    "            loss=loss,\n",
    "            eval_metric_ops={\n",
    "                'accuracy':\n",
    "                    tf.metrics.accuracy(\n",
    "                        labels=labels, predictions=tf.argmax(logits, axis=1)),\n",
    "                }\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def run_mnist(hparam, run_config):\n",
    "def run_mnist(flags_obj):\n",
    "    session_config = tf.ConfigProto(\n",
    "      inter_op_parallelism_threads=flags_obj.inter_op_parallelism_threads,\n",
    "      intra_op_parallelism_threads=flags_obj.intra_op_parallelism_threads,\n",
    "      allow_soft_placement=True)\n",
    "\n",
    "#    distribution_strategy = distribution_utils.get_distribution_strategy(\n",
    "#      distribution_strategy=flags_obj.distribution_strategy,\n",
    "#      num_gpus=flags_core.get_num_gpus(flags_obj),\n",
    "#      all_reduce_alg=flags_obj.all_reduce_alg)\n",
    "\n",
    "    run_config = tf.estimator.RunConfig(session_config=session_config)    \n",
    "    model_function = model_fn    \n",
    "    \n",
    "    data_format = None\n",
    "    if data_format is None:\n",
    "        data_format = ('channels_first'\n",
    "                   if tf.test.is_built_with_cuda() else 'channels_last')\n",
    "\n",
    "    def _metric_fn(labels, predictions):\n",
    "\n",
    "        metrics = {}\n",
    "        pred_class = predictions['class_ids']\n",
    "        metrics['micro_accuracy'] = tf.metrics.mean_per_class_accuracy(\n",
    "            labels=labels, predictions=pred_class, num_classes=NUM_CLASSES\n",
    "        )\n",
    "    \n",
    "    mnist_classifier = tf.estimator.Estimator(\n",
    "      model_fn=model_function,\n",
    "      model_dir=flags_obj.model_dir,\n",
    "      config=run_config,\n",
    "      params={\n",
    "          'data_format': data_format,\n",
    "          'learning_rate': LEARNING_RATE,\n",
    "      })\n",
    "\n",
    "    trainer = tf.contrib.estimator.add_metrics(\n",
    "        estimator=mnist_classifier, metric_fn=_metric_fn)\n",
    "\n",
    "    # Set up training and evaluation input functions.\n",
    "    def train_input_fn():\n",
    "        \"\"\"Prepare data for training.\"\"\"\n",
    "\n",
    "        # When choosing shuffle buffer sizes, larger sizes result in better\n",
    "        # randomness, while smaller sizes use less memory. MNIST is a small\n",
    "        # enough dataset that we can easily shuffle the full epoch.\n",
    "        ds = dataset.train(flags_obj.data_dir)\n",
    "        ds = ds.cache().shuffle(buffer_size=50000).batch(flags_obj.batch_size)\n",
    "\n",
    "        # Iterate through the dataset a set number (`epochs_between_evals`) of times\n",
    "        # during each training session.\n",
    "        ds = ds.repeat(flags_obj.epochs_between_evals)\n",
    "        return ds\n",
    "\n",
    "    def eval_input_fn():\n",
    "        return dataset.test(flags_obj.data_dir).batch(\n",
    "            flags_obj.batch_size).make_one_shot_iterator().get_next()    \n",
    "    \n",
    "    # Set up hook that outputs training logs every 100 steps.\n",
    "    train_hooks = hooks_helper.get_train_hooks(\n",
    "        flags_obj.hooks, model_dir=flags_obj.model_dir,\n",
    "        batch_size=flags_obj.batch_size)\n",
    "\n",
    "    # Train and evaluate model.\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)    \n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "    i = 0\n",
    "    for _ in range(flags_obj.train_epochs // flags_obj.epochs_between_evals):\n",
    "        \n",
    "        trainer.train(input_fn=train_input_fn, hooks=train_hooks)\n",
    "        \n",
    "        eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "        \n",
    "        print('\\nEvaluation results:\\n\\t%s\\n' % eval_results)\n",
    "\n",
    "        if model_helpers.past_stop_threshold(flags_obj.stop_threshold,\n",
    "                                         eval_results['accuracy']):\n",
    "              break\n",
    "        if i < 2:\n",
    "            i = i + 1\n",
    "        else:\n",
    "            break\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))    \n",
    "\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous artifacts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0429 14:32:27.714735 30860 deprecation.py:323] From c:\\users\\steve\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "I0429 14:32:27.757623 30860 estimator.py:1111] Calling model_fn.\n",
      "I0429 14:32:27.758620 30860 estimator.py:1111] Calling model_fn.\n",
      "W0429 14:32:27.861343 30860 deprecation.py:506] From c:\\users\\steve\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Experiment started at 21:32:27\n",
      ".......................................\n",
      "params: {'data_format': 'channels_last', 'learning_rate': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0429 14:32:27.951103 30860 deprecation.py:323] From c:\\users\\steve\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "I0429 14:32:28.213438 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:32:28.214400 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:32:28.216395 30860 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n",
      "I0429 14:32:28.469283 30860 monitored_session.py:222] Graph was finalized.\n",
      "I0429 14:32:28.693714 30860 session_manager.py:491] Running local_init_op.\n",
      "I0429 14:32:28.706651 30860 session_manager.py:493] Done running local_init_op.\n",
      "I0429 14:32:28.958017 30860 basic_session_run_hooks.py:594] Saving checkpoints for 0 into models\\mnist\\cnn_classifier\\model.ckpt.\n",
      "I0429 14:32:30.817341 30860 basic_session_run_hooks.py:249] cross_entropy = 2.3079066, learning_rate = 1e-04, train_accuracy = 0.1875\n",
      "I0429 14:32:30.819333 30860 basic_session_run_hooks.py:249] loss = 2.3079066, step = 1\n",
      "I0429 14:32:36.957343 30860 basic_session_run_hooks.py:680] global_step/sec: 16.284\n",
      "I0429 14:32:36.959339 30860 basic_session_run_hooks.py:247] cross_entropy = 0.95854634, learning_rate = 1e-04, train_accuracy = 0.453125 (6.142 sec)\n",
      "I0429 14:32:36.960336 30860 basic_session_run_hooks.py:247] loss = 0.95854634, step = 101 (6.141 sec)\n",
      "I0429 14:32:43.047528 30860 basic_session_run_hooks.py:680] global_step/sec: 16.4225\n",
      "I0429 14:32:43.048526 30860 basic_session_run_hooks.py:247] cross_entropy = 0.40752864, learning_rate = 1e-04, train_accuracy = 0.5833333 (6.089 sec)\n",
      "I0429 14:32:43.049523 30860 basic_session_run_hooks.py:247] loss = 0.40752864, step = 201 (6.089 sec)\n",
      "I0429 14:32:49.185120 30860 basic_session_run_hooks.py:680] global_step/sec: 16.2904\n",
      "I0429 14:32:49.187114 30860 basic_session_run_hooks.py:247] cross_entropy = 0.39366573, learning_rate = 1e-04, train_accuracy = 0.6640625 (6.139 sec)\n",
      "I0429 14:32:49.188112 30860 basic_session_run_hooks.py:247] loss = 0.39366573, step = 301 (6.139 sec)\n",
      "I0429 14:32:55.180093 30860 basic_session_run_hooks.py:680] global_step/sec: 16.6806\n",
      "I0429 14:32:55.182088 30860 basic_session_run_hooks.py:247] cross_entropy = 0.22224307, learning_rate = 1e-04, train_accuracy = 0.7125 (5.995 sec)\n",
      "I0429 14:32:55.183084 30860 basic_session_run_hooks.py:247] loss = 0.22224307, step = 401 (5.995 sec)\n",
      "I0429 14:33:01.163096 30860 basic_session_run_hooks.py:680] global_step/sec: 16.714\n",
      "I0429 14:33:01.165091 30860 basic_session_run_hooks.py:247] cross_entropy = 0.18273886, learning_rate = 1e-04, train_accuracy = 0.75 (5.983 sec)\n",
      "I0429 14:33:01.166090 30860 basic_session_run_hooks.py:247] loss = 0.18273886, step = 501 (5.983 sec)\n",
      "I0429 14:33:07.140916 30860 basic_session_run_hooks.py:680] global_step/sec: 16.7285\n",
      "I0429 14:33:07.142911 30860 basic_session_run_hooks.py:247] cross_entropy = 0.2579473, learning_rate = 1e-04, train_accuracy = 0.7723214 (5.978 sec)\n",
      "I0429 14:33:07.143909 30860 basic_session_run_hooks.py:247] loss = 0.2579473, step = 601 (5.978 sec)\n",
      "I0429 14:33:13.121926 30860 basic_session_run_hooks.py:680] global_step/sec: 16.7196\n",
      "I0429 14:33:13.123922 30860 basic_session_run_hooks.py:247] cross_entropy = 0.47338575, learning_rate = 1e-04, train_accuracy = 0.78515625 (5.981 sec)\n",
      "I0429 14:33:13.124918 30860 basic_session_run_hooks.py:247] loss = 0.47338575, step = 701 (5.981 sec)\n",
      "I0429 14:33:19.203671 30860 basic_session_run_hooks.py:680] global_step/sec: 16.4426\n",
      "I0429 14:33:19.205661 30860 basic_session_run_hooks.py:247] cross_entropy = 0.13993452, learning_rate = 1e-04, train_accuracy = 0.8020833 (6.082 sec)\n",
      "I0429 14:33:19.206659 30860 basic_session_run_hooks.py:247] loss = 0.13993452, step = 801 (6.082 sec)\n",
      "I0429 14:33:25.315327 30860 basic_session_run_hooks.py:680] global_step/sec: 16.3622\n",
      "I0429 14:33:25.317322 30860 basic_session_run_hooks.py:247] cross_entropy = 0.1584673, learning_rate = 1e-04, train_accuracy = 0.81875 (6.112 sec)\n",
      "I0429 14:33:25.318319 30860 basic_session_run_hooks.py:247] loss = 0.1584673, step = 901 (6.112 sec)\n",
      "I0429 14:33:31.350193 30860 basic_session_run_hooks.py:680] global_step/sec: 16.5704\n",
      "I0429 14:33:31.351190 30860 basic_session_run_hooks.py:247] cross_entropy = 0.10993863, learning_rate = 1e-04, train_accuracy = 0.8323864 (6.034 sec)\n",
      "I0429 14:33:31.352194 30860 basic_session_run_hooks.py:247] loss = 0.10993863, step = 1001 (6.034 sec)\n",
      "I0429 14:33:37.378142 30860 basic_session_run_hooks.py:680] global_step/sec: 16.5894\n",
      "I0429 14:33:37.379139 30860 basic_session_run_hooks.py:247] cross_entropy = 0.011961429, learning_rate = 1e-04, train_accuracy = 0.8463542 (6.028 sec)\n",
      "I0429 14:33:37.381135 30860 basic_session_run_hooks.py:247] loss = 0.011961429, step = 1101 (6.029 sec)\n",
      "I0429 14:33:43.478832 30860 basic_session_run_hooks.py:680] global_step/sec: 16.3916\n",
      "I0429 14:33:43.479829 30860 basic_session_run_hooks.py:247] cross_entropy = 0.1477952, learning_rate = 1e-04, train_accuracy = 0.85096157 (6.101 sec)\n",
      "I0429 14:33:43.481825 30860 basic_session_run_hooks.py:247] loss = 0.1477952, step = 1201 (6.101 sec)\n",
      "I0429 14:33:50.452189 30860 basic_session_run_hooks.py:680] global_step/sec: 14.3403\n",
      "I0429 14:33:50.454183 30860 basic_session_run_hooks.py:247] cross_entropy = 0.04930136, learning_rate = 1e-04, train_accuracy = 0.86160713 (6.974 sec)\n",
      "I0429 14:33:50.456178 30860 basic_session_run_hooks.py:247] loss = 0.04930136, step = 1301 (6.974 sec)\n",
      "I0429 14:33:57.339775 30860 basic_session_run_hooks.py:680] global_step/sec: 14.5189\n",
      "I0429 14:33:57.341769 30860 basic_session_run_hooks.py:247] cross_entropy = 0.04553342, learning_rate = 1e-04, train_accuracy = 0.87083334 (6.888 sec)\n",
      "I0429 14:33:57.342768 30860 basic_session_run_hooks.py:247] loss = 0.04553342, step = 1401 (6.887 sec)\n",
      "I0429 14:34:03.505291 30860 basic_session_run_hooks.py:680] global_step/sec: 16.2192\n",
      "I0429 14:34:03.507286 30860 basic_session_run_hooks.py:247] cross_entropy = 0.14648494, learning_rate = 1e-04, train_accuracy = 0.8769531 (6.166 sec)\n",
      "I0429 14:34:03.508284 30860 basic_session_run_hooks.py:247] loss = 0.14648494, step = 1501 (6.166 sec)\n",
      "I0429 14:34:09.455383 30860 basic_session_run_hooks.py:680] global_step/sec: 16.8065\n",
      "I0429 14:34:09.457377 30860 basic_session_run_hooks.py:247] cross_entropy = 0.05184496, learning_rate = 1e-04, train_accuracy = 0.88419116 (5.950 sec)\n",
      "I0429 14:34:09.458379 30860 basic_session_run_hooks.py:247] loss = 0.05184496, step = 1601 (5.950 sec)\n",
      "I0429 14:34:15.423450 30860 basic_session_run_hooks.py:680] global_step/sec: 16.7558\n",
      "I0429 14:34:15.425448 30860 basic_session_run_hooks.py:247] cross_entropy = 0.019291405, learning_rate = 1e-04, train_accuracy = 0.890625 (5.968 sec)\n",
      "I0429 14:34:15.426442 30860 basic_session_run_hooks.py:247] loss = 0.019291405, step = 1701 (5.968 sec)\n",
      "I0429 14:34:21.564033 30860 basic_session_run_hooks.py:680] global_step/sec: 16.2851\n",
      "I0429 14:34:21.566028 30860 basic_session_run_hooks.py:247] cross_entropy = 0.058905795, learning_rate = 1e-04, train_accuracy = 0.89638156 (6.141 sec)\n",
      "I0429 14:34:21.567027 30860 basic_session_run_hooks.py:247] loss = 0.058905795, step = 1801 (6.141 sec)\n",
      "I0429 14:34:26.116860 30860 basic_session_run_hooks.py:594] Saving checkpoints for 1875 into models\\mnist\\cnn_classifier\\model.ckpt.\n",
      "I0429 14:34:26.282534 30860 estimator.py:359] Loss for final step: 0.08456905.\n",
      "I0429 14:34:26.327416 30860 estimator.py:1111] Calling model_fn.\n",
      "I0429 14:34:26.474022 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:34:26.491974 30860 evaluation.py:257] Starting evaluation at 2019-04-29T21:34:26Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'data_format': 'channels_last', 'learning_rate': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:34:26.572759 30860 monitored_session.py:222] Graph was finalized.\n",
      "W0429 14:34:26.574757 30860 deprecation.py:323] From c:\\users\\steve\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0429 14:34:26.578743 30860 saver.py:1270] Restoring parameters from models\\mnist\\cnn_classifier\\model.ckpt-1875\n",
      "I0429 14:34:26.626615 30860 session_manager.py:491] Running local_init_op.\n",
      "I0429 14:34:26.637586 30860 session_manager.py:493] Done running local_init_op.\n",
      "I0429 14:34:31.910488 30860 evaluation.py:277] Finished evaluation at 2019-04-29-21:34:31\n",
      "I0429 14:34:31.911485 30860 estimator.py:1979] Saving dict for global step 1875: accuracy = 0.9738, global_step = 1875, loss = 0.081925295\n",
      "I0429 14:34:31.961157 30860 estimator.py:2039] Saving 'checkpoint_path' summary for global step 1875: models\\mnist\\cnn_classifier\\model.ckpt-1875\n",
      "I0429 14:34:32.003338 30860 estimator.py:1111] Calling model_fn.\n",
      "I0429 14:34:32.004311 30860 estimator.py:1111] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results:\n",
      "\t{'loss': 0.081925295, 'accuracy': 0.9738, 'global_step': 1875}\n",
      "\n",
      "params: {'data_format': 'channels_last', 'learning_rate': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:34:32.395261 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:34:32.396226 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:34:32.397223 30860 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n",
      "I0429 14:34:32.499981 30860 monitored_session.py:222] Graph was finalized.\n",
      "I0429 14:34:32.505966 30860 saver.py:1270] Restoring parameters from models\\mnist\\cnn_classifier\\model.ckpt-1875\n",
      "W0429 14:34:32.574750 30860 deprecation.py:323] From c:\\users\\steve\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "I0429 14:34:32.606666 30860 session_manager.py:491] Running local_init_op.\n",
      "I0429 14:34:32.620640 30860 session_manager.py:493] Done running local_init_op.\n",
      "I0429 14:34:32.863263 30860 basic_session_run_hooks.py:594] Saving checkpoints for 1875 into models\\mnist\\cnn_classifier\\model.ckpt.\n",
      "I0429 14:34:34.412446 30860 basic_session_run_hooks.py:249] cross_entropy = 0.07100821, learning_rate = 1e-04, train_accuracy = 0.96875\n",
      "I0429 14:34:34.413444 30860 basic_session_run_hooks.py:249] loss = 0.07100821, step = 1876\n",
      "I0429 14:34:40.741525 30860 basic_session_run_hooks.py:680] global_step/sec: 15.8001\n",
      "I0429 14:34:40.743519 30860 basic_session_run_hooks.py:247] cross_entropy = 0.071760364, learning_rate = 1e-04, train_accuracy = 0.96875 (6.331 sec)\n",
      "I0429 14:34:40.744522 30860 basic_session_run_hooks.py:247] loss = 0.071760364, step = 1976 (6.331 sec)\n",
      "I0429 14:34:47.029713 30860 basic_session_run_hooks.py:680] global_step/sec: 15.9028\n",
      "I0429 14:34:47.031709 30860 basic_session_run_hooks.py:247] cross_entropy = 0.083507024, learning_rate = 1e-04, train_accuracy = 0.96875 (6.288 sec)\n",
      "I0429 14:34:47.032706 30860 basic_session_run_hooks.py:247] loss = 0.083507024, step = 2076 (6.288 sec)\n",
      "I0429 14:34:53.472487 30860 basic_session_run_hooks.py:680] global_step/sec: 15.5213\n",
      "I0429 14:34:53.474483 30860 basic_session_run_hooks.py:247] cross_entropy = 0.013482147, learning_rate = 1e-04, train_accuracy = 0.9765625 (6.443 sec)\n",
      "I0429 14:34:53.475480 30860 basic_session_run_hooks.py:247] loss = 0.013482147, step = 2176 (6.443 sec)\n",
      "I0429 14:35:00.021977 30860 basic_session_run_hooks.py:680] global_step/sec: 15.2684\n",
      "I0429 14:35:00.023972 30860 basic_session_run_hooks.py:247] cross_entropy = 0.12790921, learning_rate = 1e-04, train_accuracy = 0.975 (6.549 sec)\n",
      "I0429 14:35:00.025970 30860 basic_session_run_hooks.py:247] loss = 0.12790921, step = 2276 (6.550 sec)\n",
      "I0429 14:35:07.015285 30860 basic_session_run_hooks.py:680] global_step/sec: 14.2994\n",
      "I0429 14:35:07.017278 30860 basic_session_run_hooks.py:247] cross_entropy = 0.20046374, learning_rate = 1e-04, train_accuracy = 0.96875 (6.993 sec)\n",
      "I0429 14:35:07.019272 30860 basic_session_run_hooks.py:247] loss = 0.20046374, step = 2376 (6.993 sec)\n",
      "I0429 14:35:13.398216 30860 basic_session_run_hooks.py:680] global_step/sec: 15.6668\n",
      "I0429 14:35:13.400211 30860 basic_session_run_hooks.py:247] cross_entropy = 0.03312787, learning_rate = 1e-04, train_accuracy = 0.97321427 (6.383 sec)\n",
      "I0429 14:35:13.401210 30860 basic_session_run_hooks.py:247] loss = 0.03312787, step = 2476 (6.382 sec)\n",
      "I0429 14:35:19.744250 30860 basic_session_run_hooks.py:680] global_step/sec: 15.7579\n",
      "I0429 14:35:19.746245 30860 basic_session_run_hooks.py:247] cross_entropy = 0.018631678, learning_rate = 1e-04, train_accuracy = 0.9765625 (6.346 sec)\n",
      "I0429 14:35:19.747243 30860 basic_session_run_hooks.py:247] loss = 0.018631678, step = 2576 (6.346 sec)\n",
      "I0429 14:35:26.327649 30860 basic_session_run_hooks.py:680] global_step/sec: 15.1897\n",
      "I0429 14:35:26.328647 30860 basic_session_run_hooks.py:247] cross_entropy = 0.018792726, learning_rate = 1e-04, train_accuracy = 0.9791667 (6.582 sec)\n",
      "I0429 14:35:26.330642 30860 basic_session_run_hooks.py:247] loss = 0.018792726, step = 2676 (6.583 sec)\n",
      "I0429 14:35:32.770424 30860 basic_session_run_hooks.py:680] global_step/sec: 15.5213\n",
      "I0429 14:35:32.772419 30860 basic_session_run_hooks.py:247] cross_entropy = 0.19334738, learning_rate = 1e-04, train_accuracy = 0.978125 (6.444 sec)\n",
      "I0429 14:35:32.773416 30860 basic_session_run_hooks.py:247] loss = 0.19334738, step = 2776 (6.443 sec)\n",
      "I0429 14:35:39.168886 30860 basic_session_run_hooks.py:680] global_step/sec: 15.6288\n",
      "I0429 14:35:39.170881 30860 basic_session_run_hooks.py:247] cross_entropy = 0.02811343, learning_rate = 1e-04, train_accuracy = 0.9801136 (6.398 sec)\n",
      "I0429 14:35:39.172879 30860 basic_session_run_hooks.py:247] loss = 0.02811343, step = 2876 (6.399 sec)\n",
      "I0429 14:35:45.811133 30860 basic_session_run_hooks.py:680] global_step/sec: 15.0551\n",
      "I0429 14:35:45.813127 30860 basic_session_run_hooks.py:247] cross_entropy = 0.059669472, learning_rate = 1e-04, train_accuracy = 0.9791667 (6.642 sec)\n",
      "I0429 14:35:45.814125 30860 basic_session_run_hooks.py:247] loss = 0.059669472, step = 2976 (6.641 sec)\n",
      "I0429 14:35:52.387552 30860 basic_session_run_hooks.py:680] global_step/sec: 15.2058\n",
      "I0429 14:35:52.389546 30860 basic_session_run_hooks.py:247] cross_entropy = 0.009304281, learning_rate = 1e-04, train_accuracy = 0.9807692 (6.576 sec)\n",
      "I0429 14:35:52.390552 30860 basic_session_run_hooks.py:247] loss = 0.009304281, step = 3076 (6.576 sec)\n",
      "I0429 14:35:58.639835 30860 basic_session_run_hooks.py:680] global_step/sec: 15.9942\n",
      "I0429 14:35:58.641831 30860 basic_session_run_hooks.py:247] cross_entropy = 0.034301776, learning_rate = 1e-04, train_accuracy = 0.98214287 (6.251 sec)\n",
      "I0429 14:35:58.642830 30860 basic_session_run_hooks.py:247] loss = 0.034301776, step = 3176 (6.252 sec)\n",
      "I0429 14:36:05.080617 30860 basic_session_run_hooks.py:680] global_step/sec: 15.5261\n",
      "I0429 14:36:05.082611 30860 basic_session_run_hooks.py:247] cross_entropy = 0.021851491, learning_rate = 1e-04, train_accuracy = 0.98333335 (6.442 sec)\n",
      "I0429 14:36:05.083609 30860 basic_session_run_hooks.py:247] loss = 0.021851491, step = 3276 (6.441 sec)\n",
      "I0429 14:36:11.777712 30860 basic_session_run_hooks.py:680] global_step/sec: 14.9318\n",
      "I0429 14:36:11.779714 30860 basic_session_run_hooks.py:247] cross_entropy = 0.041811675, learning_rate = 1e-04, train_accuracy = 0.984375 (6.697 sec)\n",
      "I0429 14:36:11.780704 30860 basic_session_run_hooks.py:247] loss = 0.041811675, step = 3376 (6.697 sec)\n",
      "I0429 14:36:17.995090 30860 basic_session_run_hooks.py:680] global_step/sec: 16.084\n",
      "I0429 14:36:17.997084 30860 basic_session_run_hooks.py:247] cross_entropy = 0.025906444, learning_rate = 1e-04, train_accuracy = 0.9852941 (6.217 sec)\n",
      "I0429 14:36:17.998082 30860 basic_session_run_hooks.py:247] loss = 0.025906444, step = 3476 (6.217 sec)\n",
      "I0429 14:36:24.335139 30860 basic_session_run_hooks.py:680] global_step/sec: 15.7727\n",
      "I0429 14:36:24.337139 30860 basic_session_run_hooks.py:247] cross_entropy = 0.047903866, learning_rate = 1e-04, train_accuracy = 0.984375 (6.340 sec)\n",
      "I0429 14:36:24.338132 30860 basic_session_run_hooks.py:247] loss = 0.047903866, step = 3576 (6.340 sec)\n",
      "I0429 14:36:30.920533 30860 basic_session_run_hooks.py:680] global_step/sec: 15.1851\n",
      "I0429 14:36:30.923525 30860 basic_session_run_hooks.py:247] cross_entropy = 0.011586181, learning_rate = 1e-04, train_accuracy = 0.98519737 (6.586 sec)\n",
      "I0429 14:36:30.924523 30860 basic_session_run_hooks.py:247] loss = 0.011586181, step = 3676 (6.586 sec)\n",
      "I0429 14:36:35.768572 30860 basic_session_run_hooks.py:594] Saving checkpoints for 3750 into models\\mnist\\cnn_classifier\\model.ckpt.\n",
      "I0429 14:36:35.932175 30860 estimator.py:359] Loss for final step: 0.004832561.\n",
      "I0429 14:36:35.975069 30860 estimator.py:1111] Calling model_fn.\n",
      "I0429 14:36:36.110688 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:36:36.127643 30860 evaluation.py:257] Starting evaluation at 2019-04-29T21:36:36Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'data_format': 'channels_last', 'learning_rate': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:36:36.209395 30860 monitored_session.py:222] Graph was finalized.\n",
      "I0429 14:36:36.214381 30860 saver.py:1270] Restoring parameters from models\\mnist\\cnn_classifier\\model.ckpt-3750\n",
      "I0429 14:36:36.264247 30860 session_manager.py:491] Running local_init_op.\n",
      "I0429 14:36:36.277213 30860 session_manager.py:493] Done running local_init_op.\n",
      "I0429 14:36:41.627777 30860 evaluation.py:277] Finished evaluation at 2019-04-29-21:36:41\n",
      "I0429 14:36:41.628774 30860 estimator.py:1979] Saving dict for global step 3750: accuracy = 0.9854, global_step = 3750, loss = 0.046649855\n",
      "I0429 14:36:41.632764 30860 estimator.py:2039] Saving 'checkpoint_path' summary for global step 3750: models\\mnist\\cnn_classifier\\model.ckpt-3750\n",
      "I0429 14:36:41.694599 30860 estimator.py:1111] Calling model_fn.\n",
      "I0429 14:36:41.695597 30860 estimator.py:1111] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results:\n",
      "\t{'loss': 0.046649855, 'accuracy': 0.9854, 'global_step': 3750}\n",
      "\n",
      "params: {'data_format': 'channels_last', 'learning_rate': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:36:42.138412 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:36:42.139411 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:36:42.141451 30860 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n",
      "I0429 14:36:42.244160 30860 monitored_session.py:222] Graph was finalized.\n",
      "I0429 14:36:42.250113 30860 saver.py:1270] Restoring parameters from models\\mnist\\cnn_classifier\\model.ckpt-3750\n",
      "I0429 14:36:42.350845 30860 session_manager.py:491] Running local_init_op.\n",
      "I0429 14:36:42.362812 30860 session_manager.py:493] Done running local_init_op.\n",
      "I0429 14:36:42.604201 30860 basic_session_run_hooks.py:594] Saving checkpoints for 3750 into models\\mnist\\cnn_classifier\\model.ckpt.\n",
      "I0429 14:36:44.156297 30860 basic_session_run_hooks.py:249] cross_entropy = 0.26721984, learning_rate = 1e-04, train_accuracy = 0.9375\n",
      "I0429 14:36:44.158292 30860 basic_session_run_hooks.py:249] loss = 0.26721984, step = 3751\n",
      "I0429 14:36:50.738698 30860 basic_session_run_hooks.py:680] global_step/sec: 15.192\n",
      "I0429 14:36:50.739697 30860 basic_session_run_hooks.py:247] cross_entropy = 0.05096005, learning_rate = 1e-04, train_accuracy = 0.96875 (6.583 sec)\n",
      "I0429 14:36:50.741693 30860 basic_session_run_hooks.py:247] loss = 0.05096005, step = 3851 (6.584 sec)\n",
      "I0429 14:36:57.735991 30860 basic_session_run_hooks.py:680] global_step/sec: 14.2912\n",
      "I0429 14:36:57.737986 30860 basic_session_run_hooks.py:247] cross_entropy = 0.0270612, learning_rate = 1e-04, train_accuracy = 0.96875 (6.998 sec)\n",
      "I0429 14:36:57.738986 30860 basic_session_run_hooks.py:247] loss = 0.0270612, step = 3951 (6.997 sec)\n",
      "I0429 14:37:04.702367 30860 basic_session_run_hooks.py:680] global_step/sec: 14.3547\n",
      "I0429 14:37:04.704362 30860 basic_session_run_hooks.py:247] cross_entropy = 0.058416855, learning_rate = 1e-04, train_accuracy = 0.96875 (6.965 sec)\n",
      "I0429 14:37:04.705360 30860 basic_session_run_hooks.py:247] loss = 0.058416855, step = 4051 (6.966 sec)\n",
      "I0429 14:37:11.125195 30860 basic_session_run_hooks.py:680] global_step/sec: 15.5719\n",
      "I0429 14:37:11.126193 30860 basic_session_run_hooks.py:247] cross_entropy = 0.12684603, learning_rate = 1e-04, train_accuracy = 0.96875 (6.423 sec)\n",
      "I0429 14:37:11.127191 30860 basic_session_run_hooks.py:247] loss = 0.12684603, step = 4151 (6.422 sec)\n",
      "I0429 14:37:17.765443 30860 basic_session_run_hooks.py:680] global_step/sec: 15.0574\n",
      "I0429 14:37:17.767439 30860 basic_session_run_hooks.py:247] cross_entropy = 0.011669027, learning_rate = 1e-04, train_accuracy = 0.9739583 (6.641 sec)\n",
      "I0429 14:37:17.768435 30860 basic_session_run_hooks.py:247] loss = 0.011669027, step = 4251 (6.641 sec)\n",
      "I0429 14:37:24.361808 30860 basic_session_run_hooks.py:680] global_step/sec: 15.1622\n",
      "I0429 14:37:24.362805 30860 basic_session_run_hooks.py:247] cross_entropy = 0.06376101, learning_rate = 1e-04, train_accuracy = 0.9776786 (6.595 sec)\n",
      "I0429 14:37:24.363803 30860 basic_session_run_hooks.py:247] loss = 0.06376101, step = 4351 (6.595 sec)\n",
      "I0429 14:37:30.729782 30860 basic_session_run_hooks.py:680] global_step/sec: 15.7011\n",
      "I0429 14:37:30.731777 30860 basic_session_run_hooks.py:247] cross_entropy = 0.088454336, learning_rate = 1e-04, train_accuracy = 0.9765625 (6.369 sec)\n",
      "I0429 14:37:30.731777 30860 basic_session_run_hooks.py:247] loss = 0.088454336, step = 4451 (6.368 sec)\n",
      "I0429 14:37:37.272290 30860 basic_session_run_hooks.py:680] global_step/sec: 15.287\n",
      "I0429 14:37:37.273288 30860 basic_session_run_hooks.py:247] cross_entropy = 0.08516345, learning_rate = 1e-04, train_accuracy = 0.9756944 (6.542 sec)\n",
      "I0429 14:37:37.274287 30860 basic_session_run_hooks.py:247] loss = 0.08516345, step = 4551 (6.543 sec)\n",
      "I0429 14:37:44.139142 30860 basic_session_run_hooks.py:680] global_step/sec: 14.5606\n",
      "I0429 14:37:44.140139 30860 basic_session_run_hooks.py:247] cross_entropy = 0.19228832, learning_rate = 1e-04, train_accuracy = 0.975 (6.867 sec)\n",
      "I0429 14:37:44.142135 30860 basic_session_run_hooks.py:247] loss = 0.19228832, step = 4651 (6.868 sec)\n",
      "I0429 14:37:50.358515 30860 basic_session_run_hooks.py:680] global_step/sec: 16.0788\n",
      "I0429 14:37:50.360510 30860 basic_session_run_hooks.py:247] cross_entropy = 0.013214756, learning_rate = 1e-04, train_accuracy = 0.97727275 (6.220 sec)\n",
      "I0429 14:37:50.361507 30860 basic_session_run_hooks.py:247] loss = 0.013214756, step = 4751 (6.219 sec)\n",
      "I0429 14:37:56.850160 30860 basic_session_run_hooks.py:680] global_step/sec: 15.4044\n",
      "I0429 14:37:56.852154 30860 basic_session_run_hooks.py:247] cross_entropy = 0.0049000247, learning_rate = 1e-04, train_accuracy = 0.9791667 (6.492 sec)\n",
      "I0429 14:37:56.853151 30860 basic_session_run_hooks.py:247] loss = 0.0049000247, step = 4851 (6.492 sec)\n",
      "I0429 14:38:04.884592 30860 basic_session_run_hooks.py:680] global_step/sec: 12.4464\n",
      "I0429 14:38:04.886587 30860 basic_session_run_hooks.py:247] cross_entropy = 0.013727628, learning_rate = 1e-04, train_accuracy = 0.9807692 (8.034 sec)\n",
      "I0429 14:38:04.887588 30860 basic_session_run_hooks.py:247] loss = 0.013727628, step = 4951 (8.034 sec)\n",
      "I0429 14:38:11.318617 30860 basic_session_run_hooks.py:680] global_step/sec: 15.5424\n",
      "I0429 14:38:11.319614 30860 basic_session_run_hooks.py:247] cross_entropy = 0.00390649, learning_rate = 1e-04, train_accuracy = 0.98214287 (6.433 sec)\n",
      "I0429 14:38:11.320611 30860 basic_session_run_hooks.py:247] loss = 0.00390649, step = 5051 (6.433 sec)\n",
      "I0429 14:38:19.342684 30860 basic_session_run_hooks.py:680] global_step/sec: 12.4625\n",
      "I0429 14:38:19.343681 30860 basic_session_run_hooks.py:247] cross_entropy = 0.002045403, learning_rate = 1e-04, train_accuracy = 0.98333335 (8.024 sec)\n",
      "I0429 14:38:19.344679 30860 basic_session_run_hooks.py:247] loss = 0.002045403, step = 5151 (8.024 sec)\n",
      "I0429 14:38:27.287443 30860 basic_session_run_hooks.py:680] global_step/sec: 12.5885\n",
      "I0429 14:38:27.288441 30860 basic_session_run_hooks.py:247] cross_entropy = 0.015042625, learning_rate = 1e-04, train_accuracy = 0.984375 (7.945 sec)\n",
      "I0429 14:38:27.289438 30860 basic_session_run_hooks.py:247] loss = 0.015042625, step = 5251 (7.945 sec)\n",
      "I0429 14:38:33.565659 30860 basic_session_run_hooks.py:680] global_step/sec: 15.9256\n",
      "I0429 14:38:33.567654 30860 basic_session_run_hooks.py:247] cross_entropy = 0.03338349, learning_rate = 1e-04, train_accuracy = 0.9852941 (6.279 sec)\n",
      "I0429 14:38:33.568655 30860 basic_session_run_hooks.py:247] loss = 0.03338349, step = 5351 (6.279 sec)\n",
      "I0429 14:38:39.934631 30860 basic_session_run_hooks.py:680] global_step/sec: 15.7011\n",
      "I0429 14:38:39.937624 30860 basic_session_run_hooks.py:247] cross_entropy = 0.0031561719, learning_rate = 1e-04, train_accuracy = 0.9861111 (6.370 sec)\n",
      "I0429 14:38:39.938621 30860 basic_session_run_hooks.py:247] loss = 0.0031561719, step = 5451 (6.370 sec)\n",
      "I0429 14:38:46.586847 30860 basic_session_run_hooks.py:680] global_step/sec: 15.0326\n",
      "I0429 14:38:46.587844 30860 basic_session_run_hooks.py:247] cross_entropy = 0.31258345, learning_rate = 1e-04, train_accuracy = 0.98355263 (6.650 sec)\n",
      "I0429 14:38:46.589839 30860 basic_session_run_hooks.py:247] loss = 0.31258345, step = 5551 (6.651 sec)\n",
      "I0429 14:38:51.386016 30860 basic_session_run_hooks.py:594] Saving checkpoints for 5625 into models\\mnist\\cnn_classifier\\model.ckpt.\n",
      "I0429 14:38:51.556420 30860 estimator.py:359] Loss for final step: 0.05807556.\n",
      "I0429 14:38:51.598304 30860 estimator.py:1111] Calling model_fn.\n",
      "I0429 14:38:51.737931 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:38:51.754884 30860 evaluation.py:257] Starting evaluation at 2019-04-29T21:38:51Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'data_format': 'channels_last', 'learning_rate': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:38:51.839655 30860 monitored_session.py:222] Graph was finalized.\n",
      "I0429 14:38:51.843614 30860 saver.py:1270] Restoring parameters from models\\mnist\\cnn_classifier\\model.ckpt-5625\n",
      "I0429 14:38:51.893481 30860 session_manager.py:491] Running local_init_op.\n",
      "I0429 14:38:51.904473 30860 session_manager.py:493] Done running local_init_op.\n",
      "I0429 14:38:57.276114 30860 evaluation.py:277] Finished evaluation at 2019-04-29-21:38:57\n",
      "I0429 14:38:57.277112 30860 estimator.py:1979] Saving dict for global step 5625: accuracy = 0.988, global_step = 5625, loss = 0.03834198\n",
      "I0429 14:38:57.279107 30860 estimator.py:2039] Saving 'checkpoint_path' summary for global step 5625: models\\mnist\\cnn_classifier\\model.ckpt-5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results:\n",
      "\t{'loss': 0.03834198, 'accuracy': 0.988, 'global_step': 5625}\n",
      "\n",
      ".......................................\n",
      "Experiment finished at 21:38:57\n",
      "\n",
      "Experiment elapsed time: 389.570325 seconds\n"
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    estimator = run_mnist(flags.FLAGS)\n",
    "    \n",
    "def define_mnist_flags():\n",
    "    flags_core.define_base()\n",
    "    flags_core.define_performance(num_parallel_calls=False)\n",
    "    flags_core.define_image()\n",
    "    flags.adopt_module_key_flags(flags_core)\n",
    "    flags_core.set_defaults(data_dir=datadir,\n",
    "                          model_dir=model_dir,\n",
    "                          train_epochs=40)\n",
    "    \n",
    "define_mnist_flags()\n",
    "\n",
    "if tf.gfile.Exists(model_dir):\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    tf.gfile.DeleteRecursively(model_dir)\n",
    "\n",
    "sys.argv = \"-f test\".split(\" \")\n",
    "flags.FLAGS(sys.argv)\n",
    "estimator = run_mnist(flags.FLAGS)\n",
    "#absl_app.run(main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimator = run_experiment(hparams, run_config)\n",
    "### 1.4 Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:43:13.434261 30860 estimator.py:1111] Calling model_fn.\n",
      "I0429 14:43:13.434261 30860 estimator.py:1111] Calling model_fn.\n",
      "I0429 14:43:13.594059 30860 estimator.py:1113] Done calling model_fn.\n",
      "I0429 14:43:13.596056 30860 estimator.py:1113] Done calling model_fn.\n",
      "W0429 14:43:13.600049 30860 deprecation.py:323] From c:\\users\\steve\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "I0429 14:43:13.604032 30860 export.py:587] Signatures INCLUDED in export for Eval: None\n",
      "I0429 14:43:13.607023 30860 export.py:587] Signatures INCLUDED in export for Classify: None\n",
      "I0429 14:43:13.607023 30860 export.py:587] Signatures INCLUDED in export for Train: None\n",
      "I0429 14:43:13.608020 30860 export.py:587] Signatures INCLUDED in export for Predict: ['serving_default', 'predict']\n",
      "I0429 14:43:13.609017 30860 export.py:587] Signatures INCLUDED in export for Regress: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'data_format': 'channels_last', 'learning_rate': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:43:13.645926 30860 saver.py:1270] Restoring parameters from models\\mnist\\cnn_classifier\\model.ckpt-5625\n",
      "I0429 14:43:13.690800 30860 builder_impl.py:654] Assets added to graph.\n",
      "I0429 14:43:13.691836 30860 builder_impl.py:449] No assets to write.\n",
      "I0429 14:43:13.784546 30860 builder_impl.py:414] SavedModel written to: models\\mnist\\cnn_classifier\\export\\temp-b'1556574193'\\saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'models\\\\mnist\\\\cnn_classifier\\\\export\\\\1556574193'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_dir = os.path.join(model_dir, 'export')\n",
    "    \n",
    "if tf.gfile.Exists(export_dir):\n",
    "    tf.gfile.DeleteRecursively(export_dir)\n",
    "#inputs = {'image': tf.placeholder(shape=[None,784], dtype=tf.float32, name='image')}    \n",
    "image = tf.placeholder(tf.float32, [None, 28, 28], name='image')\n",
    "input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
    "        'image': image,\n",
    "    })\n",
    "estimator.export_savedmodel(export_dir_base=export_dir, \n",
    "                            serving_input_receiver_fn=input_fn)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect the Exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1556574193\n",
      "saved_model.pb\n",
      "variables\n",
      "\r\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n",
      "\r\n",
      "signature_def['predict']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['image'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 28, 28)\r\n",
      "        name: image:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['class_ids'] tensor_info:\r\n",
      "        dtype: DT_INT64\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: strided_slice:0\r\n",
      "    outputs['classes'] tensor_info:\r\n",
      "        dtype: DT_INT64\r\n",
      "        shape: (-1)\r\n",
      "        name: ArgMax_1:0\r\n",
      "    outputs['probabilities'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 10)\r\n",
      "        name: Softmax:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n",
      "\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['image'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 28, 28)\r\n",
      "        name: image:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['class_ids'] tensor_info:\r\n",
      "        dtype: DT_INT64\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: strided_slice:0\r\n",
      "    outputs['classes'] tensor_info:\r\n",
      "        dtype: DT_INT64\r\n",
      "        shape: (-1)\r\n",
      "        name: ArgMax_1:0\r\n",
      "    outputs['probabilities'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 10)\r\n",
      "        name: Softmax:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test(saved_model_dir, signature=\"predict\", input_name='image', batch_size=300, repeat=100):\n",
    "\n",
    "#    es = dataset.test(datadir)\n",
    "#    iter = images.make_one_shot_iterator()\n",
    "    #print (\"Eval data shape: {}\".format(eval_data.shape))\n",
    "#    eval_data, eval_labels = iter.get_next()\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    \n",
    "    predictor = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir = saved_model_dir,\n",
    "        signature_def_key=signature\n",
    "    )\n",
    "    time_end = datetime.utcnow() \n",
    "        \n",
    "    time_elapsed = time_end - time_start\n",
    "   \n",
    "    print (\"\")\n",
    "    print(\"Model loading time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    print (\"\")\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    output = None\n",
    "    def eval_input_fn():\n",
    "        return dataset.test(datadir).batch(\n",
    "            batch_size).make_one_shot_iterator().get_next()       \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        for i in range(repeat):\n",
    "            input_images, labels = sess.run(eval_input_fn())        \n",
    "            #print(input_images[0])\n",
    "            input_images = input_images.reshape(batch_size, 28,28)\n",
    "            output = predictor(\n",
    "                {\n",
    "                    input_name: input_images\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    time_end = datetime.utcnow() \n",
    "\n",
    "    time_elapsed_sec = (time_end - time_start).total_seconds()\n",
    "    \n",
    "    print (\"Inference elapsed time: {} seconds\".format(time_elapsed_sec))\n",
    "    print (\"\")\n",
    "    \n",
    "    print (\"Prediction produced for {} instances batch, repeated {} times\".format(len(output['class_ids']), repeat))\n",
    "    print (\"Average latency per batch: {} seconds\".format(time_elapsed_sec/repeat))\n",
    "    print (\"\")\n",
    "    \n",
    "    print (\"Prediction output for the last instance:\")\n",
    "    for key in output.keys():\n",
    "        print (\"{}: {}\".format(key,output[key][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Prediction with SavedModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\\export\\1556574193\n",
      "\n",
      "Model loading time: 0.080784 seconds\n",
      "\n",
      "Inference elapsed time: 33.139615 seconds\n",
      "\n",
      "Prediction produced for 300 instances batch, repeated 100 times\n",
      "Average latency per batch: 0.33139615 seconds\n",
      "\n",
      "Prediction output for the last instance:\n",
      "probabilities: [3.9680742e-07 1.2645016e-06 8.4969024e-06 1.2876747e-04 2.4702018e-09\n",
      " 5.6581194e-07 7.7370957e-11 9.9983215e-01 1.2599568e-07 2.8207556e-05]\n",
      "classes: 7\n",
      "class_ids: [7]\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "print(saved_model_dir)\n",
    "inference_test(saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe GraphDef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_graph(graph_def, show_nodes=False):\n",
    "    \n",
    "    print ('Input Feature Nodes: {}'.format([node.name for node in graph_def.node if node.op=='Placeholder']))\n",
    "    print (\"\")\n",
    "    print ('Unused Nodes: {}'.format([node.name for node in graph_def.node if 'unused'  in node.name]))\n",
    "    print (\"\")\n",
    "    print ('Output Nodes: {}'.format( [node.name for node in graph_def.node if 'predictions' in node.name]))\n",
    "    print (\"\")\n",
    "    print ('Quanitization Nodes: {}'.format( [node.name for node in graph_def.node if 'quant' in node.name]))\n",
    "    print (\"\")\n",
    "    print ('Constant Count: {}'.format( len([node for node in graph_def.node if node.op=='Const'])))\n",
    "    print (\"\")\n",
    "    print ('Variable Count: {}'.format( len([node for node in graph_def.node if 'Variable' in node.op])))\n",
    "    print (\"\")\n",
    "    print ('Identity Count: {}'.format( len([node for node in graph_def.node if node.op=='Identity'])))\n",
    "    print (\"\")\n",
    "    print ('Total nodes: {}'.format( len(graph_def.node)))\n",
    "    print ('')\n",
    "    node_names = [node.name for node in graph_def.node if node.op=='Identity']\n",
    "    print(node_names)\n",
    "\n",
    "    if show_nodes==True:\n",
    "        for node in graph_def.node:\n",
    "            print ('Op:{} - Name: {}'.format(node.op, node.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Describe the SavedModel Graph (before optimisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GraphDef from a SavedModel Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_def_from_saved_model(saved_model_dir):\n",
    "    \n",
    "    print (saved_model_dir)\n",
    "    print (\"\")\n",
    "    \n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        meta_graph_def = tf.saved_model.loader.load(\n",
    "            session,\n",
    "            tags=[tag_constants.SERVING],\n",
    "            export_dir=saved_model_dir\n",
    "        )\n",
    "        \n",
    "    return meta_graph_def.graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\\export\\1556574193\n",
      "\n",
      "Input Feature Nodes: ['image', 'reshape_input']\n",
      "\n",
      "Unused Nodes: []\n",
      "\n",
      "Output Nodes: []\n",
      "\n",
      "Quanitization Nodes: []\n",
      "\n",
      "Constant Count: 57\n",
      "\n",
      "Variable Count: 41\n",
      "\n",
      "Identity Count: 16\n",
      "\n",
      "Total nodes: 225\n",
      "\n",
      "['global_step/read', 'dropout/cond/switch_t', 'dropout/cond/switch_f', 'dropout/cond/pred_id', 'dropout/cond/Identity', 'sequential/dropout/Identity', 'save/control_dependency', 'save/Identity', 'save/Identity_1', 'save/Identity_2', 'save/Identity_3', 'save/Identity_4', 'save/Identity_5', 'save/Identity_6', 'save/Identity_7', 'save/Identity_8']\n"
     ]
    }
   ],
   "source": [
    "describe_graph(get_graph_def_from_saved_model(saved_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(model_dir):\n",
    "    \n",
    "    print (model_dir)\n",
    "    print (\"\")\n",
    "    \n",
    "    pb_size = os.path.getsize(os.path.join(model_dir,'saved_model.pb'))\n",
    "    \n",
    "    variables_size = 0\n",
    "    if os.path.exists(os.path.join(model_dir,'variables/variables.data-00000-of-00001')):\n",
    "        variables_size = os.path.getsize(os.path.join(model_dir,'variables/variables.data-00000-of-00001'))\n",
    "        variables_size += os.path.getsize(os.path.join(model_dir,'variables/variables.index'))\n",
    "\n",
    "    print (\"Model size: {} KB\".format(round(pb_size/(1024.0),3)))\n",
    "    print (\"Variables size: {} KB\".format(round( variables_size/(1024.0),3)))\n",
    "    print (\"Total Size: {} KB\".format(round((pb_size + variables_size)/(1024.0),3)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\\export\\1556574193\n",
      "\n",
      "Model size: 38.92 KB\n",
      "Variables size: 6497.947 KB\n",
      "Total Size: 6536.867 KB\n"
     ]
    }
   ],
   "source": [
    "get_size(saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Freeze SavedModel\n",
    "\n",
    "This function will convert the SavedModel into a GraphDef file (freezed_model.pb), and storing the variables as constrant to the freezed_model.pb\n",
    "\n",
    "You need to define the graph output nodes for freezing. We are only interested in the **class_id**, which is produced by **head/predictions/ExpandDims** node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(saved_model_dir):\n",
    "    \n",
    "    from tensorflow.python.tools import freeze_graph\n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "    \n",
    "    output_graph_filename = os.path.join(saved_model_dir, \"freezed_model.pb\")\n",
    "#    output_node_names = \"head/predictions/ExpandDims\"\n",
    "#    output_node_names = \"PREDICT/predictions/probabilities\"\n",
    "    output_node_names = \"strided_slice\"\n",
    "    \n",
    "    initializer_nodes = \"\"\n",
    "\n",
    "    freeze_graph.freeze_graph(\n",
    "        input_saved_model_dir=saved_model_dir,\n",
    "        output_graph=output_graph_filename,\n",
    "        saved_model_tags = tag_constants.SERVING,\n",
    "        output_node_names=output_node_names,\n",
    "        initializer_nodes=initializer_nodes,\n",
    "\n",
    "        input_graph=None, \n",
    "        input_saver=False,\n",
    "        input_binary=False, \n",
    "        input_checkpoint=None, \n",
    "        restore_op_name=None, \n",
    "        filename_tensor_name=None, \n",
    "        clear_devices=False,\n",
    "        input_meta_graph=False,\n",
    "    )\n",
    "    \n",
    "    print (\"SavedModel graph freezed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel graph freezed!\n"
     ]
    }
   ],
   "source": [
    "node_names = [node.name for node in tf.get_default_graph().as_graph_def().node]\n",
    "\n",
    "freeze_graph(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1556574193\n",
      "freezed_model.pb\n",
      "saved_model.pb\n",
      "variables\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Describe the freezed_model.pb Graph (after freezing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GraphDef from GraphDef File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_def_from_file(graph_filepath):\n",
    "    \n",
    "    print (graph_filepath)\n",
    "    print (\"\")\n",
    "    \n",
    "    from tensorflow.python import ops\n",
    "    \n",
    "    with ops.Graph().as_default():\n",
    "        with tf.gfile.GFile(graph_filepath, \"rb\") as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            \n",
    "            return graph_def\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\\export\\1556574193\\freezed_model.pb\n",
      "\n",
      "Input Feature Nodes: ['image']\n",
      "\n",
      "Unused Nodes: []\n",
      "\n",
      "Output Nodes: []\n",
      "\n",
      "Quanitization Nodes: []\n",
      "\n",
      "Constant Count: 22\n",
      "\n",
      "Variable Count: 0\n",
      "\n",
      "Identity Count: 9\n",
      "\n",
      "Total nodes: 55\n",
      "\n",
      "['sequential/conv2d/Conv2D/ReadVariableOp', 'sequential/conv2d/BiasAdd/ReadVariableOp', 'sequential/conv2d_1/Conv2D/ReadVariableOp', 'sequential/conv2d_1/BiasAdd/ReadVariableOp', 'sequential/dense/MatMul/ReadVariableOp', 'sequential/dense/BiasAdd/ReadVariableOp', 'sequential/dropout/Identity', 'sequential/dense_1/MatMul/ReadVariableOp', 'sequential/dense_1/BiasAdd/ReadVariableOp']\n"
     ]
    }
   ],
   "source": [
    "freezed_filepath=os.path.join(saved_model_dir,'freezed_model.pb')\n",
    "describe_graph(get_graph_def_from_file(freezed_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimise the freezed_model.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise GraphDef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_graph(model_dir, graph_filename, transforms):\n",
    "    \n",
    "    from tensorflow.tools.graph_transforms import TransformGraph\n",
    "    \n",
    "    input_names = []\n",
    "#    output_names = ['head/predictions/ExpandDims']\n",
    "    output_names = ['strided_slice']\n",
    "    \n",
    "    graph_def = get_graph_def_from_file(os.path.join(model_dir, graph_filename))\n",
    "    optimised_graph_def = TransformGraph(graph_def, \n",
    "                                         input_names,\n",
    "                                         output_names,\n",
    "                                         transforms \n",
    "                                        )\n",
    "    tf.train.write_graph(optimised_graph_def,\n",
    "                        logdir=model_dir,\n",
    "                        as_text=False,\n",
    "                        name='optimised_model.pb')\n",
    "    \n",
    "    print (\"Freezed graph optimised!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\\export\\1556574193\\freezed_model.pb\n",
      "\n",
      "Freezed graph optimised!\n"
     ]
    }
   ],
   "source": [
    "transforms = [\n",
    "    'remove_nodes(op=Identity)', \n",
    "    'fold_constants(ignore_errors=true)',\n",
    "    'fold_batch_norms',\n",
    "#    'fuse_resize_pad_and_conv',\n",
    "#    'quantize_weights',\n",
    "#    'quantize_nodes',\n",
    "    'merge_duplicate_nodes',\n",
    "    'strip_unused_nodes', \n",
    "    'sort_by_execution_order'\n",
    "]\n",
    "\n",
    "optimize_graph(saved_model_dir, 'freezed_model.pb', transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mnist/cnn_classifier/export/1556574193\n",
      "freezed_model.pb\n",
      "optimised_model.pb\n",
      "saved_model.pb\n",
      "variables\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Describe the Optimised Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\\export\\1556574193\\optimised_model.pb\n",
      "\n",
      "Input Feature Nodes: ['image']\n",
      "\n",
      "Unused Nodes: []\n",
      "\n",
      "Output Nodes: []\n",
      "\n",
      "Quanitization Nodes: []\n",
      "\n",
      "Constant Count: 15\n",
      "\n",
      "Variable Count: 0\n",
      "\n",
      "Identity Count: 0\n",
      "\n",
      "Total nodes: 39\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "optimised_filepath=os.path.join(saved_model_dir,'optimised_model.pb')\n",
    "describe_graph(get_graph_def_from_file(optimised_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert Optimised graph (GraphDef) to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_graph_def_to_saved_model(graph_filepath):\n",
    "\n",
    "    from tensorflow.python import ops\n",
    "    export_dir=os.path.join(saved_model_dir,'optimised')\n",
    "\n",
    "    if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "    graph_def = get_graph_def_from_file(graph_filepath)\n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as session:\n",
    "        tf.import_graph_def(graph_def, name=\"\")\n",
    "        tf.saved_model.simple_save(session,\n",
    "                export_dir,\n",
    "                inputs={\n",
    "                    node.name: session.graph.get_tensor_by_name(\"{}:0\".format(node.name)) \n",
    "                    for node in graph_def.node if node.op=='Placeholder'},\n",
    "                outputs={\n",
    "                    \"class_ids\": session.graph.get_tensor_by_name(\"strided_slice:0\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print (\"Optimised graph converted to SavedModel!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\\export\\1556558269\\optimised_model.pb\n",
      "\n",
      "Optimised graph converted to SavedModel!\n"
     ]
    }
   ],
   "source": [
    "optimised_filepath=os.path.join(saved_model_dir,'optimised_model.pb')\n",
    "convert_graph_def_to_saved_model(optimised_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimised SavedModel Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\\export\\1556558269\\optimised\n",
      "\n",
      "Model size: 12799.12 KB\n",
      "Variables size: 0.0 KB\n",
      "Total Size: 12799.12 KB\n"
     ]
    }
   ],
   "source": [
    "optimised_saved_model_dir = os.path.join(saved_model_dir,'optimised') \n",
    "get_size(optimised_saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_model.pb\n",
      "variables\n",
      "\r\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n",
      "\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['image'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 28, 28)\r\n",
      "        name: image:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['class_ids'] tensor_info:\r\n",
      "        dtype: DT_INT64\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: strided_slice:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)/optimised\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir ${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prediction with the Optimised SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\mnist\\cnn_classifier\\export\\1556558269\\optimised\n",
      "\n",
      "Model loading time: 0.163525 seconds\n",
      "\n",
      "Inference elapsed time: 71.615196 seconds\n",
      "\n",
      "Prediction produced for 300 instances batch, repeated 100 times\n",
      "Average latency per batch: 0.7161519599999999 seconds\n",
      "\n",
      "Prediction output for the last instance:\n",
      "class_ids: [7]\n"
     ]
    }
   ],
   "source": [
    "optimised_saved_model_dir = os.path.join(saved_model_dir,'optimised') \n",
    "print(optimised_saved_model_dir)\n",
    "inference_test(saved_model_dir=optimised_saved_model_dir, signature='serving_default', input_name='image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud ML Engine Deployment and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'steven-wang-playground'\n",
    "BUCKET = 'steven-gcs-cloudml'\n",
    "REGION = 'europe-west1'\n",
    "MODEL_NAME = 'mnist_classifier'\n",
    "\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload the model artefacts to Google Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m rm -r gs://${BUCKET}/tf-model-optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "\n",
    "echo ${saved_model_dir}\n",
    "\n",
    "gsutil -m cp -r ${saved_model_dir} gs://${BUCKET}/tf-model-optimisation/original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=models/mnist/cnn_classifier/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)/optimised\n",
    "\n",
    "echo ${saved_model_dir}\n",
    "\n",
    "gsutil -m cp -r ${saved_model_dir} gs://${BUCKET}/tf-model-optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploy models to Cloud ML Engine\n",
    "\n",
    "Don't forget to delete the model and the model version if they were previously deployed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo ${MODEL_NAME}\n",
    "\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions=${REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version: v_org** is the original SavedModel (before optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_VERSION='v_org'\n",
    "MODEL_ORIGIN=gs://${BUCKET}/tf-model-optimisation/original\n",
    "\n",
    "gcloud ml-engine versions create ${MODEL_VERSION}\\\n",
    "            --model=${MODEL_NAME} \\\n",
    "            --origin=${MODEL_ORIGIN} \\\n",
    "            --runtime-version=1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version: v_opt** is the optimised SavedModel (after optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_VERSION='v_opt'\n",
    "MODEL_ORIGIN=gs://${BUCKET}/tf-model-optimisation/optimised\n",
    "\n",
    "gcloud ml-engine versions create ${MODEL_VERSION}\\\n",
    "            --model=${MODEL_NAME} \\\n",
    "            --origin=${MODEL_ORIGIN} \\\n",
    "            --runtime-version=1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cloud ML Engine online predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build(\n",
    "    'ml', 'v1', \n",
    "    credentials=credentials, \n",
    "    discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json'\n",
    ")\n",
    "\n",
    "    \n",
    "def predict(version, instances):\n",
    "\n",
    "    request_data = {'instances': instances}\n",
    "\n",
    "    model_url = 'projects/{}/models/{}/versions/{}'.format(PROJECT, MODEL_NAME, version)\n",
    "    response = api.projects().predict(body=request_data, name=model_url).execute()\n",
    "\n",
    "    class_ids = None\n",
    "    \n",
    "    try:\n",
    "        class_ids = [item[\"class_ids\"] for item in response[\"predictions\"]]\n",
    "    except:\n",
    "        print response\n",
    "    \n",
    "    return class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_cmle(version, batch=100, repeat=10):\n",
    "    \n",
    "    instances = [\n",
    "            {'input_image': [float(i) for i in list(eval_data[img])] }\n",
    "        for img in range(batch)\n",
    "    ]\n",
    "\n",
    "    #warmup request\n",
    "    predict(version, instances[0])\n",
    "    print 'Warm up request performed!'\n",
    "    print 'Timer started...'\n",
    "    print ''\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    output = None\n",
    "    \n",
    "    for i in range(repeat):\n",
    "        output = predict(version, instances)\n",
    "    \n",
    "    time_end = datetime.utcnow() \n",
    "\n",
    "    time_elapsed_sec = (time_end - time_start).total_seconds()\n",
    "    \n",
    "    print \"Inference elapsed time: {} seconds\".format(time_elapsed_sec)\n",
    "    print \"\"\n",
    "    \n",
    "    print \"Prediction produced for {} instances batch, repeated {} times\".format(len(output), repeat)\n",
    "    print \"Average latency per batch: {} seconds\".format(time_elapsed_sec/repeat)\n",
    "    print \"\"\n",
    "    \n",
    "    print \"Prediction output for the last instance: {}\".format(output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version='v_org'\n",
    "inference_cmle(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version='v_opt'\n",
    "inference_cmle(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Happy serving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
